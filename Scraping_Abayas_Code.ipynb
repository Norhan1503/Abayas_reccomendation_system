{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3GZwU8Wq63R"
      },
      "outputs": [],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import csv\n",
        "import json\n"
      ],
      "metadata": {
        "id": "zAeJpEV0rCj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALHareem website\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "class Scraper:\n",
        "    def __init__(self, base_url, folder_path, csv_file_path, max_pages,category):\n",
        "        # Initialize WebDriver options\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "        # Store base URL and file paths\n",
        "        self.base_url = base_url\n",
        "        self.folder_path = folder_path\n",
        "        os.makedirs(self.folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
        "        self.csv_file_path = csv_file_path\n",
        "        self.max_pages = max_pages\n",
        "        self.category=category\n",
        "        self.data = []\n",
        "\n",
        "    def extract_data(self):\n",
        "        page_number = 1\n",
        "        while page_number <= self.max_pages:\n",
        "            self.driver.get(f\"https://hijabulhareem.com/collections/abaya-jilbab?page={page_number}\")\n",
        "            time.sleep(5)\n",
        "\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            items = soup.find(\"div\", {\"class\": \"on_list_view_false products nt_products_holder row fl_center row_pr_2 cdt_des_1 round_cd_false nt_contain ratio_nt position_8 space_30 nt_default\"})\n",
        "\n",
        "            print(f\"Number of items found on page {page_number}: {len(items)}\")\n",
        "\n",
        "            if not items:\n",
        "                print(\"No items found. Ending pagination.\")\n",
        "                break\n",
        "\n",
        "            for idx, item in enumerate(items):\n",
        "              try:\n",
        "                # Extract product name\n",
        "                name_tag = item.find(\"h3\", class_=\"product-title pr fs__14 mg__0 fwm\")\n",
        "                product_name = name_tag.find(\"a\").get_text(strip=True) if name_tag else None\n",
        "\n",
        "\n",
        "                # Extract product price\n",
        "                price = round(float(item.select_one(\"span.price\").text.replace(\"Rs.\",\"\").replace(\",\",\"\")) / 230, 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                # Extract product ID\n",
        "                product_id_tag = item.find(\"div\", attrs={\"data-id\": True})\n",
        "                product_id = product_id_tag[\"data-id\"] if product_id_tag else None\n",
        "\n",
        "\n",
        "                #Extract product link\n",
        "                product_link = item.select_one(\"h3.product-title a\")[\"href\"]\n",
        "                product_link = \"https://hijabulhareem.com\" + product_link\n",
        "\n",
        "\n",
        "                # p_img (أعلى جودة)\n",
        "                img = item.select_one(\"div.pr_lazy_img.main-img\")\n",
        "                bgset = img[\"data-bgset\"]\n",
        "                p_img = bgset.split(\",\")[-1].split()[0]\n",
        "                if p_img.startswith(\"//\"):\n",
        "                  image_url = \"https:\" + p_img\n",
        "                else:\n",
        "                  image_url = p_img\n",
        "\n",
        "\n",
        "                if product_id and product_name and price and image_url and product_link:\n",
        "                  # Store extracted data\n",
        "                  self.data.append({\n",
        "                  'p_id': product_id,\n",
        "                  'source': 'AlHreem',\n",
        "                  'p_base_price': price,\n",
        "                  'p_name': product_name,\n",
        "                  'category': self.category,\n",
        "                  'p_gender': 'Women',\n",
        "                  'p_img': image_url,\n",
        "                  'p_link': product_link})\n",
        "\n",
        "                  print(f\"Page {page_number}, Item {idx}: Image URL: {image_url}, Price: {price}, Name: {product_name}\")\n",
        "                else:\n",
        "                  print(f\"Item {idx} on page {page_number} skipped due to missing data.\")\n",
        "              except :\n",
        "                pass\n",
        "\n",
        "            page_number += 1\n",
        "\n",
        "\n",
        "    def save_data_to_csv(self):\n",
        "        df = pd.DataFrame(self.data)\n",
        "        if os.path.exists(self.csv_file_path):\n",
        "            df.to_csv(self.csv_file_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            df.to_csv(self.csv_file_path, index=False)\n",
        "        print(\"Data saved to CSV.\")\n",
        "\n",
        "    def close(self):\n",
        "        self.driver.quit()\n",
        "\n",
        "\n",
        "folder_path = '/content/drive/My Drive/Abayas'\n",
        "csv_file_path = '/content/drive/My Drive/Abayas/Abayas_dataset5.csv'\n",
        "\n",
        "categories={\n",
        "    'Abayas':6\n",
        "\n",
        "}\n",
        "for key,value in categories.items():\n",
        "\n",
        "  url=f\"https://hijabulhareem.com/collections/abaya-jilbab?page={value}\"\n",
        "  scraper = Scraper(url, folder_path, csv_file_path,value,key)\n",
        "  scraper.extract_data()\n",
        "  scraper.save_data_to_csv()\n",
        "  scraper.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUL9vC2krIRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Noir Website\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "class Scraper:\n",
        "    def __init__(self, base_url, folder_path, csv_file_path, max_pages,category):\n",
        "        # Initialize WebDriver options\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "        # Store base URL and file paths\n",
        "        self.base_url = base_url\n",
        "        self.folder_path = folder_path\n",
        "        os.makedirs(self.folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
        "        self.csv_file_path = csv_file_path\n",
        "        self.max_pages = max_pages\n",
        "        self.category=category\n",
        "        self.data = []\n",
        "\n",
        "    def extract_data(self):\n",
        "        page_number = 1\n",
        "        while page_number <= self.max_pages:\n",
        "            self.driver.get(f\"https://abayanoir.com/product-category/abaya/page/{page_number}/?v=fbe46383db39\")\n",
        "            time.sleep(5)\n",
        "\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            items = soup.find(\"div\", {\"class\": \"products wd-products wd-grid-g grid-columns-3 elements-grid pagination-pagination title-line-two wd-stretch-cont-lg wd-products-with-shadow\"})\n",
        "\n",
        "            print(f\"Number of items found on page {page_number}: {len(items)}\")\n",
        "\n",
        "            if not items:\n",
        "                print(\"No items found. Ending pagination.\")\n",
        "                break\n",
        "\n",
        "            for idx, item in enumerate(items):\n",
        "              try:\n",
        "                # Extract product name\n",
        "                title_tag = item.find(\"h3\", class_=\"wd-entities-title\")\n",
        "                if title_tag:\n",
        "                   product_name = title_tag.get_text(strip=True)\n",
        "                else:\n",
        "                   product_name = None\n",
        "\n",
        "                # Extract product price\n",
        "                price_tag = item.find(\"span\", class_=\"woocommerce-Price-amount amount\")\n",
        "                price = price_tag.get_text(strip=True) if price_tag else None\n",
        "\n",
        "\n",
        "                # Extract product ID\n",
        "                product_id = item.get(\"data-id\")\n",
        "\n",
        "                # Extract image\n",
        "                image_url = soup.find(\"img\")[\"src\"]\n",
        "\n",
        "                # Ensure full URL (add \"https:\" if missing)\n",
        "                image_tag = item.find(\"img\")\n",
        "                if image_tag:\n",
        "                   if \"srcset\" in image_tag.attrs:\n",
        "                       image_url = image_tag[\"srcset\"].split(\",\")[0].split(\" \")[0].strip()\n",
        "                   else:\n",
        "                       image_url = image_tag[\"src\"]\n",
        "                else:\n",
        "                   image_url = None\n",
        "\n",
        "                # Extract product link\n",
        "                try:\n",
        "                  product_link = item.find(\"a\")[\"href\"]   # Direct link from the <a> tag\n",
        "                  print(\"Product Link:\", product_link)\n",
        "                except:\n",
        "                  product_link =None\n",
        "\n",
        "\n",
        "                if product_id and product_name and price and image_url and product_link:\n",
        "                  # Store extracted data\n",
        "                  self.data.append({\n",
        "                  'p_id': product_id,\n",
        "                  'source': 'Noir',\n",
        "                  'p_base_price': price,\n",
        "                  'p_name': product_name,\n",
        "                  'category': self.category,\n",
        "                  'p_gender': 'Women',\n",
        "                  'p_img': image_url,\n",
        "                  'p_link': product_link})\n",
        "\n",
        "                  print(f\"Page {page_number}, Item {idx}: Image URL: {image_url}, Price: {original_price}, Name: {product_name}\")\n",
        "                else:\n",
        "                  print(f\"Item {idx} on page {page_number} skipped due to missing data.\")\n",
        "              except :\n",
        "                pass\n",
        "\n",
        "            page_number += 1\n",
        "\n",
        "\n",
        "    def save_data_to_csv(self):\n",
        "        df = pd.DataFrame(self.data)\n",
        "        if os.path.exists(self.csv_file_path):\n",
        "            df.to_csv(self.csv_file_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            df.to_csv(self.csv_file_path, index=False)\n",
        "        print(\"Data saved to CSV.\")\n",
        "\n",
        "    def close(self):\n",
        "        self.driver.quit()\n",
        "\n",
        "# if you want any others categories change url and no. of pages\n",
        "folder_path = '/content/drive/My Drive/Abayas'\n",
        "csv_file_path = '/content/drive/My Drive/Abayas/Abayas_dataset.csv'\n",
        "\n",
        "categories={\n",
        "    'Abayas':6\n",
        "\n",
        "}\n",
        "for key,value in categories.items():\n",
        "\n",
        "  url=f\"https://abayanoir.com/product-category/abaya/page/{value}/?v=fbe46383db39\"\n",
        "  scraper = Scraper(url, folder_path, csv_file_path,value,key)\n",
        "  scraper.extract_data()\n",
        "  scraper.save_data_to_csv()\n",
        "  scraper.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZRKpZcIrRnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dhaagy wbsite\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "class Scraper:\n",
        "    def __init__(self, base_url, folder_path, csv_file_path, max_pages,category):\n",
        "        # Initialize WebDriver options\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "        # Store base URL and file paths\n",
        "        self.base_url = base_url\n",
        "        self.folder_path = folder_path\n",
        "        os.makedirs(self.folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
        "        self.csv_file_path = csv_file_path\n",
        "        self.max_pages = max_pages\n",
        "        self.category=category\n",
        "        self.data = []\n",
        "\n",
        "    def extract_data(self):\n",
        "        page_number = 1\n",
        "        while page_number <= self.max_pages:\n",
        "            self.driver.get(f\"https://www.dhaagey.pk/collections/abayas?page={page_number}\")\n",
        "            time.sleep(5)\n",
        "\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            items = soup.find(\"div\", {\"class\": \"m-collection-products m:flex m:flex-wrap m-cols-4\"})\n",
        "\n",
        "            print(f\"Number of items found on page {page_number}: {len(items)}\")\n",
        "\n",
        "            if not items:\n",
        "                print(\"No items found. Ending pagination.\")\n",
        "                break\n",
        "\n",
        "            for idx, item in enumerate(items):\n",
        "              try:\n",
        "                # Extract product name\n",
        "                h3_tag = item.find(\"h3\", class_=\"m-product-card__title\")\n",
        "                if h3_tag:\n",
        "                  a_tag = h3_tag.find(\"a\")\n",
        "                  product_name = a_tag.get_text(strip=True) if a_tag else None\n",
        "                else:\n",
        "                  product_name = None\n",
        "\n",
        "\n",
        "                # Extract product price\n",
        "                price_text = item.find(\"span\", class_=\"m-price-item--last\").get_text(strip=True)\n",
        "                # إزالة العلامة Rs وأي فواصل، وتحويل الرقم لفلوات\n",
        "                price_number = float(price_text.replace(\"Rs.\",\"\").replace(\",\",\"\"))\n",
        "                # تحويل للعملة (مثلاً لو 1 دولار = 82 روبية هندية)\n",
        "                price = round(price_number / 82, 2)\n",
        "\n",
        "                #Extract ID\n",
        "                product_id_div = item.find(\"div\", class_=\"m-product-card\")\n",
        "                if not product_id_div:\n",
        "                    product_id_div = item\n",
        "                product_id = product_id_div.get(\"data-product-id\")\n",
        "                print(\"Product ID:\", product_id)\n",
        "\n",
        "                # Extract main product image\n",
        "                image_div = item.find(\"div\", class_=\"m-product-card__main-image\")\n",
        "                if image_div:\n",
        "                    img_tag = image_div.find(\"img\")\n",
        "                    if img_tag and img_tag.get(\"src\"):\n",
        "                       image_url = img_tag[\"src\"]\n",
        "                       if image_url.startswith(\"//\"):\n",
        "                          image_url = \"https:\" + image_url\n",
        "                       print(\"Image Link:\", image_url)\n",
        "                    else:\n",
        "                       print(\"No image found\")\n",
        "                else:\n",
        "                    print(\"No image div found\")\n",
        "\n",
        "                # Extract product link\n",
        "                p_link = item.find(\"a\", class_=\"m-product-card__link\")[\"href\"]\n",
        "                product_link = \"https://www.dhaagey.pk\" + p_link\n",
        "\n",
        "\n",
        "\n",
        "                if product_id and product_name and price and image_url and product_link:\n",
        "                  # Store extracted data\n",
        "                  self.data.append({\n",
        "                  'p_id': product_id,\n",
        "                  'source': 'Dhaagy',\n",
        "                  'p_base_price': price,\n",
        "                  'p_name': product_name,\n",
        "                  'category': self.category,\n",
        "                  'p_gender': 'Women',\n",
        "                  'p_img': image_url,\n",
        "                  'p_link': product_link})\n",
        "\n",
        "                  print(f\"Page {page_number}, Item {idx}: Image URL: {image_url}, Price: {price}, Name: {product_name}\")\n",
        "                else:\n",
        "                  print(f\"Item {idx} on page {page_number} skipped due to missing data.\")\n",
        "              except :\n",
        "                pass\n",
        "\n",
        "            page_number += 1\n",
        "\n",
        "\n",
        "    def save_data_to_csv(self):\n",
        "        df = pd.DataFrame(self.data)\n",
        "        if os.path.exists(self.csv_file_path):\n",
        "            df.to_csv(self.csv_file_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            df.to_csv(self.csv_file_path, index=False)\n",
        "        print(\"Data saved to CSV.\")\n",
        "\n",
        "    def close(self):\n",
        "        self.driver.quit()\n",
        "\n",
        "# if you want any others categories change url and no. of pages\n",
        "folder_path = '/content/drive/My Drive/Abayas'\n",
        "csv_file_path = '/content/drive/My Drive/Abayas/Abayas_dataset3.csv'\n",
        "\n",
        "categories={\n",
        "    'Abayas':5\n",
        "\n",
        "}\n",
        "for key,value in categories.items():\n",
        "\n",
        "  url=f\"https://www.dhaagey.pk/collections/abayas?page={value}\"\n",
        "  scraper = Scraper(url, folder_path, csv_file_path,value,key)\n",
        "  scraper.extract_data()\n",
        "  scraper.save_data_to_csv()\n",
        "  scraper.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "MUUjVzIQrhaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# malbus website\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "class Scraper:\n",
        "    def __init__(self, base_url, folder_path, csv_file_path, max_pages,category):\n",
        "        # Initialize WebDriver options\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "        # Store base URL and file paths\n",
        "        self.base_url = base_url\n",
        "        self.folder_path = folder_path\n",
        "        os.makedirs(self.folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
        "        self.csv_file_path = csv_file_path\n",
        "        self.max_pages = max_pages\n",
        "        self.category=category\n",
        "        self.data = []\n",
        "\n",
        "    def extract_data(self):\n",
        "        page_number = 1\n",
        "        while page_number <= self.max_pages:\n",
        "            self.driver.get(f\"https://www.bymalbus.com/collections/abaya?page={page_number}\")\n",
        "            time.sleep(5)\n",
        "\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            items = soup.find(\"div\", {\"class\": \"collection__grid\"})\n",
        "\n",
        "            print(f\"Number of items found on page {page_number}: {len(items)}\")\n",
        "\n",
        "            if not items:\n",
        "                print(\"No items found. Ending pagination.\")\n",
        "                break\n",
        "\n",
        "            for idx, item in enumerate(items):\n",
        "              try:\n",
        "                # Extract product name\n",
        "                a_tag = item.find(\"h3\", class_=\"card__title\").find(\"a\") if item.find(\"h3\", class_=\"card__title\") else None\n",
        "                product_name = a_tag.get_text(strip=True) if a_tag else None\n",
        "\n",
        "\n",
        "                # Extract product price\n",
        "                price_tag = item.find(\"span\", class_=\"money bacurr-money\")\n",
        "                price = price_tag.get_text(strip=True) if price_tag else None\n",
        "\n",
        "\n",
        "                # Extract image\n",
        "                image_url = soup.find(\"img\")[\"src\"]\n",
        "\n",
        "                image_url = (item.find(\"img\", class_=\"card__img card__img--hover\")\n",
        "                .get(\"srcset\", \"\")\n",
        "                .split(\",\")[-1]\n",
        "                .split()[0]\n",
        "                .replace(\"//\", \"https://\"))\n",
        "\n",
        "\n",
        "                # Extract product link\n",
        "                link_tag = item.find(\"a\", class_=\"card card--left\")\n",
        "                product_link = link_tag.get(\"href\") if link_tag else None\n",
        "\n",
        "                # لو الرابط relative (\" /products/... \") نحوله لرابط كامل\n",
        "                if product_link and product_link.startswith(\"/\"):\n",
        "                     product_link = \"https://www.bymalbus.com\" + product_link\n",
        "\n",
        "\n",
        "                if product_name and price and image_url and product_link:\n",
        "                  # Store extracted data\n",
        "                  self.data.append({\n",
        "\n",
        "                  'source': 'malbus',\n",
        "                  'p_base_price': price,\n",
        "                  'p_name': product_name,\n",
        "                  'category': self.category,\n",
        "                  'p_gender': 'Women',\n",
        "                  'p_img': image_url,\n",
        "                  'p_link': product_link})\n",
        "\n",
        "                  print(f\"Page {page_number}, Item {idx}: Image URL: {image_url}, Price: {price}, Name: {product_name}\")\n",
        "                else:\n",
        "                  print(f\"Item {idx} on page {page_number} skipped due to missing data.\")\n",
        "              except :\n",
        "                pass\n",
        "\n",
        "            page_number += 1\n",
        "\n",
        "\n",
        "    def save_data_to_csv(self):\n",
        "        df = pd.DataFrame(self.data)\n",
        "        if os.path.exists(self.csv_file_path):\n",
        "            df.to_csv(self.csv_file_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            df.to_csv(self.csv_file_path, index=False)\n",
        "        print(\"Data saved to CSV.\")\n",
        "\n",
        "    def close(self):\n",
        "        self.driver.quit()\n",
        "\n",
        "# if you want any others categories change url and no. of pages\n",
        "folder_path = '/content/drive/My Drive/Abayas'\n",
        "csv_file_path = '/content/drive/My Drive/Abayas/Abayas_dataset2.csv'\n",
        "\n",
        "categories={\n",
        "    'Abayas':3\n",
        "\n",
        "}\n",
        "for key,value in categories.items():\n",
        "\n",
        "  url=f\"https://www.bymalbus.com/collections/abaya?page={value}\"\n",
        "  scraper = Scraper(url, folder_path, csv_file_path,value,key)\n",
        "  scraper.extract_data()\n",
        "  scraper.save_data_to_csv()\n",
        "  scraper.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "PA6AtstQrvDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zeinab Butterfly website\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "class Scraper:\n",
        "    def __init__(self, base_url, folder_path, csv_file_path, max_pages,category):\n",
        "        # Initialize WebDriver options\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "        # Store base URL and file paths\n",
        "        self.base_url = base_url\n",
        "        self.folder_path = folder_path\n",
        "        os.makedirs(self.folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
        "        self.csv_file_path = csv_file_path\n",
        "        self.max_pages = max_pages\n",
        "        self.category=category\n",
        "        self.data = []\n",
        "\n",
        "    def extract_data(self):\n",
        "        page_number = 1\n",
        "        while page_number <= self.max_pages:\n",
        "            self.driver.get(f\"https://zainabcollection.com/product-category/butterfly-abaya/page/{page_number}/\")\n",
        "            time.sleep(5)\n",
        "\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            items = soup.find(\"div\", {\"class\": \"products wd-products wd-grid-g grid-columns-3 elements-grid pagination-pagination\"})\n",
        "\n",
        "            print(f\"Number of items found on page {page_number}: {len(items)}\")\n",
        "\n",
        "            if not items:\n",
        "                print(\"No items found. Ending pagination.\")\n",
        "                break\n",
        "\n",
        "            for idx, item in enumerate(items):\n",
        "              try:\n",
        "                # Extract product name\n",
        "                name_tag = item.find(\"h3\", class_=\"wd-entities-title\")\n",
        "                product_name = name_tag.find(\"a\").get_text(strip=True) if name_tag else None\n",
        "\n",
        "\n",
        "                # Extract product price\n",
        "                price_tag = item.find(\"span\", class_=\"price\")\n",
        "                price_rs = price_tag.get_text(strip=True) if price_tag else None\n",
        "\n",
        "                if price_rs:\n",
        "                  # حذف الرمز والفواصل وتحويل النص لرقم\n",
        "                  price_number = float(price_rs.replace(\"₨\", \"\").replace(\"\\xa0\", \"\").replace(\",\", \"\").strip())\n",
        "                  # التحويل للدولار\n",
        "                  conversion_rate = 230  # مثال: 1 دولار ≈ 230 Rs\n",
        "                  price = round(price_number / conversion_rate, 2)\n",
        "\n",
        "\n",
        "                # Extract product ID\n",
        "                product_id = item.get(\"data-id\")  # من div الرئيسي\n",
        "\n",
        "\n",
        "\n",
        "                # Extract image URL\n",
        "                img_tag = item.find(\"img\")\n",
        "                image_url = img_tag.get(\"data-src\") or img_tag.get(\"src\") if img_tag else None\n",
        "\n",
        "                # Ensure full URL (add \"https:\" if missing)\n",
        "                if image_url.startswith(\"//\"):\n",
        "                     image_url = \"https:\" + image_url\n",
        "                     print(\"Image Link:\", image_url)\n",
        "                # Extract product link\n",
        "                link_tag = item.find(\"a\", class_=\"product-image-link\")\n",
        "                product_link = link_tag.get(\"href\") if link_tag else None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                if product_id and product_name and price and image_url and product_link:\n",
        "                  # Store extracted data\n",
        "                  self.data.append({\n",
        "                  'p_id': product_id,\n",
        "                  'source': 'Butterfly',\n",
        "                  'p_base_price': price,\n",
        "                  'p_name': product_name,\n",
        "                  'category': self.category,\n",
        "                  'p_gender': 'Women',\n",
        "                  'p_img': image_url,\n",
        "                  'p_link': product_link})\n",
        "\n",
        "                  print(f\"Page {page_number}, Item {idx}: Image URL: {image_url}, Price: {price}, Name: {product_name}\")\n",
        "                else:\n",
        "                  print(f\"Item {idx} on page {page_number} skipped due to missing data.\")\n",
        "              except :\n",
        "                pass\n",
        "\n",
        "            page_number += 1\n",
        "\n",
        "\n",
        "    def save_data_to_csv(self):\n",
        "        df = pd.DataFrame(self.data)\n",
        "        if os.path.exists(self.csv_file_path):\n",
        "            df.to_csv(self.csv_file_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            df.to_csv(self.csv_file_path, index=False)\n",
        "        print(\"Data saved to CSV.\")\n",
        "\n",
        "    def close(self):\n",
        "        self.driver.quit()\n",
        "\n",
        "# if you want any others categories change url and no. of pages\n",
        "folder_path = '/content/drive/My Drive/Abayas'\n",
        "csv_file_path = '/content/drive/My Drive/Abayas/Abayas_dataset4.csv'\n",
        "\n",
        "categories={\n",
        "    'Abayas':5\n",
        "\n",
        "}\n",
        "for key,value in categories.items():\n",
        "\n",
        "  url=f\"https://zainabcollection.com/product-category/butterfly-abaya/page/{value}/\"\n",
        "  scraper = Scraper(url, folder_path, csv_file_path,value,key)\n",
        "  scraper.extract_data()\n",
        "  scraper.save_data_to_csv()\n",
        "  scraper.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "b7_dWJ08sCWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}